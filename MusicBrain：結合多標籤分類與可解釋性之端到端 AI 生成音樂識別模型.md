---
title: MusicBrain：結合多標籤分類與可解釋性之端到端 AI 生成音樂識別模型

---

# MusicBrain：結合多標籤分類與可解釋性之端到端 AI 生成音樂識別模型
# 壹、前言
## 摘要

本研究針對 AI 生成音樂（Machine-Generated Music Detection, MGMD）領域，提出一套名為 MusicBrain 的可解釋性端到端模型。隨著生成技術如 Suno、NotaGen 的普及，著作權歸屬與原創保護成為亟待解決的法律與倫理課題。本研究核心目標在於利用**多標籤分類（Multi-label Classification）** 與 **可解釋人工智慧（XAI）** 技術，精準標註生成音樂中的素材來源、曲風特徵及作曲家風格。實驗架構採用 ConvNeXt 作為特徵提取骨幹，並導入**多尺度注意力模組（Multi-Scale Attention Module）** 以強化模型透明度。研究結果不僅能辨識音樂是否由 AI 生成，更透過頻譜權重分析提供具備物理意義的歸因解釋。

---

## 一.研究動機


隨著生成式 AI 技術的跨越式發展，音樂創作門檻大幅降低，廣泛應用於影視、遊戲與廣告產業。然而，其背後隱藏的版權倫理挑戰日益嚴峻，多數模型訓練數據源於受版權保護的既有作品，導致生成產物在法律定位與侵權認定上存在灰色地帶。相較於視覺影像，音樂特徵（如節奏脈動、和弦進行、音色微動）更具隱蔽性，難以透過感官直接辨識相似性。因此，建立一套透明且可追溯的分析機制，對於保障原創者權益與建立產業秩序至關重要。


## 二 .研究目的

本研究旨在建構一套可解釋的模型框架，具體研究目標包含：

1.  **AI 生成判定**：精確區別 AI 生成音訊與人類原創作品。
2. **多維度風格辨識**：運用 Multi-label Classification 技術，同時標記多種音樂風格與編曲特徵。
3. **歸因可解釋性**：透過辨識權重分布，量化分析模型決策依據，說明其參考之音樂元素。

# 貳、論文回顧
## 多標籤分類與可解釋性模型

* **多標籤分類 (Multi-label Classification)**：針對單一樣本包含多重屬性的特性（如音樂中同時具備「古典」與「鋼琴」標籤）進行建模，與傳統單一類別分類器相比，更能反映現實複雜度。 
* **可解釋性模型 (Explainable AI)**：強調決策過程透明化。本研究使用**熱圖（Heatmap）**，確保特徵權重具備可觀察性。

## ARTBRAIN 之技術啟發
本研究借鑒前人針對藝術作品提出的 ARTBRAIN 工具套件思路。ARTBRAIN 利用多標籤識別與熱圖（Heatmap）技術對圖像進行風格歸因，其整合解釋機制與端到端流程的設計，為本研究在音訊領域的實踐提供了模型架構之參考價值。


# 參、研究設備及器材

## 3.1 硬體資源
實驗於配備 Intel Xeon Gold 5118 CPU 與雙 RTX 5000 GPU 之伺服器端進行，使用 PyTorch 深度學習框架開發。

# 肆、研究過程與系統架構

## 4.1 資料集預處理

### 資料集來源:
本研究選用具備**高度穩定性與法律合規性**的古典音樂數據集進行訓練：

**MusicNet MIDI**：包含 330 首專業標註之古典室內樂作品，用於捕捉人類作曲之結構規律。

**NotaGen 生成音訊**：使用基於 LLM 架構之生成權重（包含 12 層與 20 層等不同深度模型），生成符合特定時期、作曲家風格的 AI 音訊作為對照組。


### 4.2 深度神經網路架構設計

本系統採用 ConvNeXt 作為特徵提取骨幹，並劃分為三階段處理流程：
1. **低層特徵區塊 (Low-level)**：捕捉音韻基礎與頻譜紋理。
2. **中層特徵區塊 (Mid-level)**：提取旋律進行與節奏模式。 
3. **高層特徵區塊 (High-level)**：建模抽象語意與藝術風格特徵。


### 4.3 多尺度注意力模組 (Multi-Scale Attention)

此模組為本研究核心創新點，旨在實現不同特徵層級的動態權重配置：
* **特徵融合**：將三階段特徵圖於通道軸串接，生成融合張量 $Z_{concat} \in \mathbb{R}^{H \times W \times C}$。
* **SE-Blocks 機制**：透過全域平均池化（Global Average Pooling）進行壓縮（Squeeze），並利用雙層全連接網絡搭配 ReLU 與 Sigmoid 激活函數進行激勵（Excitation），產出通道重要性權重 $\omega$。
* **特徵重構**：將權重 $\omega$ 作用於原始特徵，強化關鍵特徵區域並弱化噪訊。


# 伍、實驗設計與評估方法

## 5.1 訓練策略

依照ArtBrain的啟發，我麼涉及了以下訓練策略:
* 優化器：使用 Adam 優化器，初始學習率設為 0.001。
* 學習率調度：導入驗證損失監控機制，若連續兩個 Epoch 未改善則自動調降學習率至 10%。
* 可解釋性分析：採用 FM-G-CAM 技術，將特徵權重反向映射至聲譜圖，以視覺化方式呈現決策依據。

本研究針對「多標籤音樂分類」與「模型解釋性」兩大核心，建立了一套多維度的評估體系，以驗證模型在複雜音訊背景下辨識多種樂器或曲風的準確性與合理性。

## 5.2 效能評估指標
1. **平均精確率均值 (mAP)**：評估模型對多類標籤的整體檢索效能。
2. **解釋性忠實度 (Fidelity)**：透過「刪除實驗 (Deletion)」移除高權重時頻特徵，觀測模型置信度之下降程度，以驗證解釋之真實性。
3. **領域對齊評估 (Alignment)**：
    * 頻域定位準確度：比對模型關注之頻譜與樂器基頻、泛音之物理符合度。
    * 時序顯著性分析：觀察模型在節奏突變處（如鼓點切入）的注意力波動。